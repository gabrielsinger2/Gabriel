Here are two different implementations (in Python) to solve the polygonal isoperimetric inequality for triangles and hexagons.
We use gradient descent (coded almost from scratch) and the Scipy library. 


Algo number 1 : 

import numpy as np
import matplotlib.pyplot as plt 
import matplotlib.pyplot as plt
import seaborn as sns

#This is the perimeter function.
def Per(X):
    a=np.asarray(X)
    x,y,z=a[0],a[1],a[2]
    return (x + np.sqrt(y**2 + z**2) + np.sqrt((y - x)**2 + z**2))
def PER(X):
    #rectangle is: A=(0,0);B=(x,0),C=(y,z),D=(0,w)
    X=np.asarray(X)
    x,y,z,w=X[0],X[1],X[2],X[3]
    return np.abs(np.sqrt(X[0]))+np.sqrt((x-y)**2+z**2)+np.sqrt(y**2+(z-w)**2)+np.sqrt(z**2)
#This is the aera function:
def AIRE(X):
    X=np.asarray(X)
    x,y,z,w=X[0],X[1],X[2],X[3]
    return 1/2 *(x*z+w*y)
#This is the isoperimetric quotient, the function that we want to minimize. 
def RAPPORT(X):
    return (PER(X))**2/AIRE(X)
def Rapport(x):
    a=np.asarray(x)
    x,y,z=a[0],a[1],a[2]
    return 2*((x +np.sqrt(y**2+z**2)+np.sqrt((y-x)**2+z**2)))**2/(x*z)
#implémentation of the gradient of the objective function.
def Grad(X):
    a = np.asarray(X)
    grad = np.zeros_like(a)
    x, y, z = a[0], a[1], a[2]
    grad[0] = (4 * ((x - y) / np.sqrt((x - y) ** 2 + z ** 2) + 1) * (np.sqrt((x - y) ** 2 + z ** 2) + x + np.sqrt(y ** 2 + z ** 2))) / (x * z) - (2 * (np.sqrt((x - y) ** 2 + z ** 2) + x + np.sqrt(y ** 2 + z ** 2)) ** 2) / (x ** 2 * z)
    grad[1] = (4 * ((y - x) / np.sqrt((x - y) ** 2 + z ** 2) + y / np.sqrt(y ** 2 + z ** 2)) * (np.sqrt((x - y) ** 2 + z ** 2) + x + np.sqrt(y ** 2 + z ** 2))) / (x * z)
    grad[2] = (4 * (z / np.sqrt((x - y) ** 2 + z ** 2) + z / np.sqrt(y ** 2 + z ** 2)) * (np.sqrt((x - y) ** 2 + z ** 2) + x + np.sqrt(y ** 2 + z ** 2))) / (x * z) - (2 * (np.sqrt((x - y) ** 2 + z ** 2) + x + np.sqrt(y ** 2 + z ** 2)) ** 2) / (x * z ** 2)
    return grad


#Implementation of the (deterministic) gradient descent with an constant step.
def steepest_descent_constant_step(f, grad, x0, iterations, error_point, error_grad, h):    
    dim = np.max(np.shape(x0))
    x_list = np.zeros([dim,iterations])
    f_list = np.zeros(iterations)
    error_point_list = np.zeros(iterations)
    error_grad_list = np.zeros(iterations)
    x = np.asarray(x0)
    x_old = x
    grad_x = grad(x)
    #ploting triangles 
    fig, axs = plt.subplots(8)
    for i in range(iterations):
        #x = x - h*grad(x)/np.linalg.norm(grad(x))
        x = x - h*grad(x)
        grad_x = grad(x)
        f_x = f(x)
        x_list[:,i] = x
        f_list[i] = f_x
        error_point_list[i] = np.linalg.norm(x - x_old)
        error_grad_list[i] = np.linalg.norm(grad_x)
        
        if i % 1000 == 0:
            X,y,z=x[0],x[1],x[2]
            abscisse, ordonnées=[0,X,y,0],[0,0,z,0]  
            fig1=plt.figure(1)
            ax1=fig1.add_subplot(111)
            #Plot Triangle 1 
            ax1.axis('square')
            plt.plot(abscisse,ordonnées,color=[0/255,176/255,80/255])
            #adapting the axes of each plot. 
            plt.xlim(min(abscisse)-1,max(abscisse)+1)
            plt.ylim(min(ordonnées)-1,max(ordonnées)+1)
            plt.show()
        #if (error_grad_list[i] < error_grad):
         #   break
          #  x_old = x

        if (error_point_list[i] < error_point)|(error_grad_list[i] < error_grad):
            break
        x_old = x    
    print('h',h ,'coordonnées',x, f(x),dim)
    
    #print ("point error={}, grad error={}, iteration={}, f(x)={}".format(error_point_list[i], error_grad_list[i],i+1,f(x)))
    #return { 'x_list' : x_list[:,0:i] , 'f_list' : f_list[0:i], 'error_point_list' : error_point_list[0:i], 'error_point_list' : error_point_list[0:i]}
f=Rapport
grad=Grad
x0 = np.array([8,-1,1])
iterations = 10000
error_point = 1e-10
error_grad = 1e-10
h = 1e-3
result = steepest_descent_constant_step(f, grad, x0, iterations, error_point, error_grad, h)



Algo number 2 : 
import numpy as np
import matplotlib.pyplot as plt
import scipy

#We compute perimeter and the aera function.
def P(X):
    A=np.asarray(X)
    a,b,c,d,e,f=A[0],A[1],A[2],A[3],A[4],A[5]
    return 1+np.sqrt((a-1)**2+b**2)+np.sqrt((c-a)**2+(d-b)**2)+np.sqrt((e-c)**2+(d-f)**2)+np.sqrt(e**2+f**2)
def A(X):
    A=np.asarray(X)
    a,b,c,d,e,f=A[0],A[1],A[2],A[3],A[4],A[5]
    return (1/2)*abs((-b+b*c-d*a+d*e-c*f))
    #return (abs(a)+abs(b*c)-abs(d*a)+abs(d*e)-abs(c*f))*(1/2)
def RR(X):
    return P(X)**2/A(X)
#scipy.optimize.minimize(RR,[1,1,9,1,1,1/3])

def isop(x_0=[1,2,3,4,1,56]):
    result=scipy.optimize.minimize(RR,x_0)
    x_optimal=result.x
    a,b,c,d,e,f=x_optimal[0],x_optimal[1],x_optimal[2],x_optimal[3],x_optimal[4],x_optimal[5]
    abscisse, ordonnées=[0,1,a,c,e,0],[0,0,b,d,f,0] #On doit fermer la figuer d ou les six points 
    fig1=plt.figure(1)
    ax1=fig1.add_subplot(111)
    # Plot pentagone 1 
    ax1.axis('square')
    plt.plot(abscisse,ordonnées,color=[0/255,176/255,80/255,80/255])
    #plot one optimum (there is no unicity)
    plt.xlim(min(abscisse)-1,max(abscisse)+1)
    plt.ylim(min(ordonnées)-1,max(ordonnées)+1)
    plt.show()
    delta=result.fun-5*4*np.tan(np.pi/5)
    print('The difference between the theoritical value and numerical value m* is', delta,'.')
    print(result)
